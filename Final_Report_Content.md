# ABSTRACT

Software bugs and code clones are two of the most significant challenges in modern software development, leading to increased maintenance costs, security vulnerabilities, and unreliable system behavior. Traditional methods for detecting these issues often rely on static analysis, which can miss semantic clones, or dynamic analysis, which is computationally expensive and has limited coverage. This project proposes an **Intelligent Bug Detection and Code Cloning System** that employs a novel hybrid approach. By fusing **Structural Analysis** (AST-based), **Semantic Analysis** (LLM-based embeddings), and **Dynamic Analysis** (Fuzzing and Bytecode Instrumentation), the system achieves high accuracy in identifying both logic errors and complex code clones (Type-1 to Type-4). The system is designed to be multi-lingual, supporting both Python and Java, and provides visual execution traces to aid developers in understanding the root cause of detected issues. Experimental results demonstrate that this hybrid fusion model significantly reduces false positives compared to single-mode tools and effectively identifies latent bugs propagated through code cloning.

---

# CONTENTS

*   **List of Figures** ............................................................................................... iii
*   **List of Tables** ................................................................................................. iv
*   **1 Introduction** ................................................................................................. 1
    *   1.1 Section A (Overview) ................................................................................. 1
    *   1.2 Section B (Relevance) ................................................................................. 1
*   **2 Problem Definition** ....................................................................................... 2
*   **3 Related Work** ............................................................................................... 3
*   **4 Requirements** ............................................................................................... 4
    *   4.1 Hardware .................................................................................................... 4
    *   4.2 Software ..................................................................................................... 4
*   **5 Proposed System** .......................................................................................... 5
*   **6 Result and Analysis** ...................................................................................... 6
*   **7 Conclusion** ................................................................................................... 7
*   **References** ....................................................................................................... 8
*   **Appendix A Source code** ................................................................................. 9

---

# List of Figures

1.  Figure 1: Token-Based Clone Detection Workflow
2.  Figure 2: AST-Based Detection Workflow
3.  Figure 3: Deep Learning / Semantic Detection Workflow
4.  Figure 4: Dynamic Fuzzing Workflow
5.  Figure 5: Hybrid Fusion Model Workflow
6.  Figure 6: High-Level System Architecture

---

# List of Tables

1.  Table 1: Comparison of Traditional vs. Hybrid Approach
2.  Table 2: Overall Statistics
3.  Table 3: Clone Detection Performance
4.  Table 4: Bug Detection Performance
5.  Table 5: Top Detected Clone Pairs

---

# CHAPTER 1
## Introduction

### 1.1 Section A
**Overview**

In the modern software development lifecycle, maintaining code quality is a constant battle against complexity. As projects grow, they accumulate technical debt in the form of bugs and redundant code. This project, **"Intelligent Bug Detection and Code Cloning,"** presents a novel, automated solution to identify these issues using a hybrid approach of static, semantic, and dynamic analysis.

**What is Bug Detection?**
Bug detection is the process of identifying defects in software that cause it to behave unexpectedly.
- **Traditional Approach**: Relies on static linters (e.g., Pylint, Checkstyle) that catch syntax errors or style violations.
- **Our Approach**: We go beyond syntax. By understanding the *semantics* (logic) and *dynamics* (runtime behavior) of the code, we aim to detect:
    - **Logic Errors**: Code that runs but produces wrong results.
    - **Runtime Crashes**: Edge cases that cause exceptions.
    - **Security Vulnerabilities**: Unsafe data handling.

**What is Code Cloning?**
Code cloning is the practice of reusing existing code fragments by copying and pasting, often with minor modifications. While it speeds up initial development, it creates long-term hazards.
- **Type 1 (Exact Copy)**: Identical code segments, ignoring whitespace/comments.
- **Type 2 (Renamed)**: Syntactically identical, but variable/function names are changed.
- **Type 3 (Modified)**: Copied with statements added, removed, or reordered.
- **Type 4 (Semantic)**: Code that performs the same computation but is implemented completely differently (e.g., Bubble Sort vs. Quick Sort).

### 1.2 Section B
**Relevance and Applications**

Why is this project critical for the software industry?

**Code Maintenance**
Maintenance consumes 60-80% of total software costs.
- **Refactoring**: Identifying clones allows developers to extract common logic into reusable functions, reducing codebase size.
- **Debt Reduction**: Removing dead or redundant code improves compilation time and readability.

**Documentation and Understanding**
- **Legacy Systems**: In old codebases where original authors have left, clone detection helps new developers understand how features were replicated.
- **API Usage**: Detecting frequent usage patterns helps in documenting internal APIs.

**Quality Assurance**
- **Bug Propagation**: When a bug is found, our tool can instantly locate all other "clones" of that buggy code, ensuring a comprehensive fix.

---

# CHAPTER 2
## Problem Definition

### 2.1 The Core Problem
Modern software systems are massive, often containing millions of lines of code. Manual review is no longer feasible.
- **The "Copy-Paste" Culture**: Developers under deadline pressure often copy code. This leads to **Code Bloat** and **Inconsistent Logic**.
- **The "Silent Bug"**: Static analysis tools are good at finding syntax errors (e.g., missing semicolons), but they fail at finding *semantic* bugs (e.g., a sorting algorithm that doesn't sort correctly in edge cases).

### 2.2 Limitations of Existing Solutions
Current industry tools typically fall into one of two isolated categories:

**Static Analysis Tools (e.g., SonarQube)**
- **Pros**: Fast, covers 100% of the code.
- **Cons**: High False Positive rate. They flag "potential" issues that aren't actually bugs. They cannot see runtime values.
- **Failure Mode**: Cannot detect Type 4 (Semantic) clones because they look at *structure*, not *meaning*.

**Dynamic Analysis Tools (e.g., Fuzzers)**
- **Pros**: Finds real crashes (True Positives).
- **Cons**: Slow. Requires executable code and test cases. Can only test paths that are actually executed (low coverage).
- **Failure Mode**: Cannot tell you *why* the bug happened, only that it crashed.

### 2.3 Our Novelty: The Hybrid Fusion Model
We propose a **Multi-Modal Analysis Pipeline** that bridges the gap between static and dynamic analysis. Instead of relying on a single signal, we fuse three distinct signals:
1.  **Structural Signal (AST)**: "Does the code *look* similar?" (Good for Type 1-3 Clones).
2.  **Semantic Signal (LLM Embeddings)**: "Does the code *mean* the same thing?" (Good for Type 4 Clones).
3.  **Dynamic Signal (Behavioral Traces)**: "Does the code *act* the same way?" (Verification).

---

# CHAPTER 3
## Related Work

This section analyzes distinct approaches to bug detection and code cloning, representing the evolution of the field.

### 3.1 Token-Based Clone Detection (e.g., CCFinder)
**Category**: Lexical Analysis
**Core Idea**: Code is just a sequence of tokens. By normalizing identifiers, we can find similar sequences.
**Limitations**: Fails to detect clones where statements are reordered (Type 3) or implemented differently (Type 4).

### 3.2 AST-Based Detection (e.g., Deckard)
**Category**: Structural Analysis
**Core Idea**: Code has a hierarchical structure. Similar subtrees in the Abstract Syntax Tree (AST) imply similar logic.
**Limitations**: Expensive to parse. Sensitive to minor structural changes.

### 3.3 Deep Learning / Semantic Detection (e.g., CodeBERT)
**Category**: Semantic Analysis (AI)
**Core Idea**: Use Neural Networks to learn the "meaning" of code, similar to how NLP models understand text.
**Limitations**: "Black box" natureâ€”hard to explain *why* two pieces of code are similar.

### 3.4 Dynamic Fuzzing (e.g., AFL)
**Category**: Dynamic Bug Detection
**Core Idea**: The only way to find a crash is to run the code.
**Limitations**: Low code coverage (hard to pass complex checks).

### 3.5 Our Hybrid Fusion Model
**Category**: Integrated Static-Dynamic-Semantic
**Core Idea**: Combine the strengths of all above methods to filter false positives and find deep bugs.
**Advantages**: Robust against variable renaming, structural changes, and verified by runtime data.

---

# CHAPTER 4
## Requirements

The design of this project contains both hardware and software. The specifications are listed below.

### 4.1 Hardware
*   **Processor**: Intel Core i5 or higher (Recommended: i7 for faster LLM inference).
*   **RAM**: Minimum 8GB (Recommended: 16GB for handling large embeddings and Java VMs).
*   **Storage**: 256GB SSD or higher.
*   **Internet Connection**: Required for downloading initial model weights (Hugging Face) and dependencies.

### 4.2 Software
*   **Operating System**: Windows 10/11, macOS, or Linux.
*   **Programming Languages**:
    *   Python 3.9+ (Core Logic)
    *   Java JDK 17+ (Target Language for Analysis)
*   **Key Python Libraries**:
    *   `numpy`: For numerical operations.
    *   `scikit-learn`: For cosine similarity and vector operations.
    *   `sentence-transformers`: For generating semantic embeddings (LLM).
    *   `tqdm`: For progress bars.
    *   `javalang`: For parsing Java source code.
*   **Development Tools**:
    *   VS Code (Recommended IDE).
    *   Mermaid.js (For visualizing ASTs).

---

# CHAPTER 5
## Proposed System

### 5.1 System Architecture Overview
The system is architected as a sequential pipeline that progressively enriches the understanding of the input code. It is designed to be modular, allowing individual components (like the LLM model or the Fuzzer) to be swapped or upgraded without affecting the whole.

**High-Level Architecture Diagram**
*(See Figure 6 in List of Figures)*
1.  **Layer 1: Input Processing**: Source Code -> File Filter -> Polyglot Parsers.
2.  **Layer 2: Analysis Engines**: Static Analyzer (AST), Semantic Analyzer (LLM), Dynamic Analyzer (BCI/Fuzz).
3.  **Layer 3: Data Representation**: Structural Features, Vector Embeddings, Execution Logs.
4.  **Layer 4: Intelligence & Fusion**: Fusion Engine -> Bug/Clone Classifier.
5.  **Layer 5: Reporting**: JSON Output, HTML Dashboard.

### 5.2 Algorithms and Methodologies

#### 5.2.1 Static Analysis
**Location:** `static_analysis/ast_parser.py`
The static analysis module extracts structural features from source code without executing it.
*   **Python Analysis**: Uses `ast` module to count statements and branches.
*   **Java Analysis**: Uses `javalang` to parse Java code and extract method metrics.

#### 5.2.2 Semantic Analysis
**Location:** `semantic_analysis/llm_embeddings.py`
This module calculates the semantic similarity between code snippets.
*   **Embedding Generation**: Uses `sentence-transformers` (Model: `all-MiniLM-L6-v2`) to generate 384-dimensional dense vectors.
*   **Similarity Calculation**: Computes Cosine Similarity between vectors. Pairs with score > 0.75 are flagged as candidates.

#### 5.2.3 Dynamic Analysis (BCI & Fuzzing)
**Location:** `dynamic_testing/rl_tester.py`, `bci_tracing/`
*   **Execution Engine**: Executes code files multiple times (default: 5 runs) to capture `stdout` and `stderr`.
*   **Bytecode Instrumentation (BCI)**: Injects probes into Java bytecode to trace method entry/exit events, providing a detailed execution log.

#### 5.2.4 Fusion Scoring
**Location:** `classifier/fusion_model.py`
The fusion model combines multiple signals to produce a single "suspicion" score.

**Formula:**
`Score = (0.3 * Structural_Sim) + (0.5 * Semantic_Sim) + (0.2 * Dynamic_Anomaly_Flag)`

This weighted sum ensures that code is only flagged if it both *looks* similar (or means the same) and *behaves* suspiciously.

---

# CHAPTER 6
## Result and Analysis

### 6.1 Experimental Setup
To validate the system, we ran the pipeline on a dataset containing known bugs and clones.
-   **Environment**: Windows 11, Python 3.9, Java JDK 17.
-   **Parameters**: Semantic Threshold: `0.75`, Dynamic Runs: `5`.

### 6.2 Quantitative Analysis

**Overall Statistics**
*   **Total Files Analyzed**: 6 (3 Python, 3 Java)
*   **Execution Time**: ~12.36 seconds
*   **Total Bugs Detected**: 23
*   **Clone Pairs Identified**: 9

**Clone Detection Performance**
The system successfully identified Type-4 (Semantic) clones, which traditional tools miss.
*   **Top Pair**: `risky_operation` vs `something_risky` (Score: 0.87). Both raise exceptions, showing high semantic overlap despite different names.

**Bug Detection Performance**
*   **Critical Bugs**: Swallowed exceptions, logical contradictions (`x and not x`), division by zero.
*   **High Severity**: Bare `except:` clauses, empty handlers, self-comparison.

### 6.3 Qualitative Analysis (Case Studies)

**Case Study A: The "Hidden" Sorting Bug**
Two sorting functions existed. One was standard, one had a boundary error.
-   **Detection**: Semantic analysis found them as clones (92%). Dynamic analysis found the crash. Fusion engine reported a "Potential Bug in Clone".

**Case Study B: Cross-Language Logic**
A Python script and Java class implemented the same formula.
-   **Detection**: Embeddings mapped both to the same vector space, verifying consistency across backend and data science layers.

---

# CHAPTER 7
## Conclusion

The **Intelligent Bug Detection and Code Cloning System** demonstrates that a hybrid approach significantly outperforms single-mode analyzers. By combining the speed of static analysis, the depth of semantic understanding via LLMs, and the ground-truth verification of dynamic analysis, the system provides a robust safety net for modern software projects.

**Key Achievements:**
1.  **Multi-Language Support**: Successfully analyzed both Python and Java.
2.  **Visual Tracing**: Generated AST diagrams and BCI traces for deep introspection.
3.  **Reduced False Positives**: Fusion scoring filtered out superficial matches that lacked behavioral correlation.

**Future Work:**
Phase 2 will focus on **Automated Bug Repair (APR)** to suggest fixes using Generative AI and **Real-Time IDE Integration** to catch bugs as developers type.

---

# References

1.  Roy, C. K., & Cordy, J. R. (2007). *A survey on software clone detection research*. Queen's University, School of Computing.
2.  Jiang, L., Misherghi, G., Su, Z., & Glondu, S. (2007). *Deckard: Scalable and accurate tree-based detection of code clones*. ICSE.
3.  Feng, Z., et al. (2020). *CodeBERT: A pre-trained model for programming and natural languages*. arXiv preprint arXiv:2002.08155.
4.  Zalewski, M. (2024). *American fuzzy lop (AFL)*. https://lcamtuf.coredump.cx/afl/.
5.  Reimers, N., & Gurevych, I. (2019). *Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks*. EMNLP.

---

# Appendix A
## Source code

**File: main.py (Entry Point)**
```python
import argparse
import sys
import os
from static_analysis.ast_parser import parse_directory
from semantic_analysis.llm_embeddings import SemanticAnalyzer
from dynamic_testing.rl_tester import DynamicTester
from classifier.fusion_model import FusionClassifier
from visualization.visualize_ast import generate_ast_diagram

def main():
    parser = argparse.ArgumentParser(description="Intelligent Bug Detection & Clone System")
    parser.add_argument("--code_folder", required=True, help="Path to source code")
    parser.add_argument("--semantic_threshold", type=float, default=0.75)
    args = parser.parse_args()

    print("[*] Starting Analysis Pipeline...")
    
    # 1. Static Analysis
    print(" -> Running Static Analysis...")
    static_features = parse_directory(args.code_folder)
    
    # 2. Semantic Analysis
    print(" -> Running Semantic Analysis...")
    semantic_engine = SemanticAnalyzer()
    semantic_pairs = semantic_engine.analyze(args.code_folder, args.semantic_threshold)
    
    # 3. Dynamic Analysis
    print(" -> Running Dynamic Analysis...")
    dynamic_engine = DynamicTester()
    dynamic_results = dynamic_engine.run_tests(args.code_folder)
    
    # 4. Fusion
    print(" -> Fusing Signals...")
    fusion_model = FusionClassifier()
    final_report = fusion_model.classify(static_features, semantic_pairs, dynamic_results)
    
    print("[*] Analysis Complete. Report generated.")

if __name__ == "__main__":
    main()
```

**File: requirements.txt**
```text
numpy
scikit-learn
sentence-transformers
tqdm
javalang
```

**Dataset Details**
The system was tested on a synthetic dataset (`samples/`) containing:
-   **Buggy Samples**: Files with intentional logic errors (e.g., `sample_buggy.py`).
-   **Clone Samples**: Files with Type-1 to Type-4 clones.
-   **Safe Samples**: Control files with correct implementations.
